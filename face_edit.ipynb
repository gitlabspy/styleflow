{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_edit",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFBWduuFKDJD"
      },
      "source": [
        "!git clone https://github.com/gitlabspy/stylegan2-tf-2.x.git\n",
        "!gdown --id 1T2GwCxz_vAZY2us4xnljZyIgosfSN9rn\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!rm -r /content/stylegan2-tf-2.x/official-converted/ \n",
        "!unzip  official-converted.zip \n",
        "!rm -r __MACOSX/ \n",
        "!mv /content/official-converted/ /content/stylegan2-tf-2.x/\n",
        "!mkdir /content/stylegan2-tf-2.x/raw_images\n",
        "!mkdir /content/stylegan2-tf-2.x/aligned\n",
        "!mv /content/shape_predictor_68_face_landmarks.dat.bz2 /content/stylegan2-tf-2.x/\n",
        "%cd /content/stylegan2-tf-2.x/\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCK8R8niKDO-"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from stylegan2.utils import postprocess_images\n",
        "from load_models import load_generator\n",
        "from copy_official_weights import convert_official_weights_together\n",
        "from tf_utils import allow_memory_growth\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto, InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = InteractiveSession(config=config)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ckpt_dir_cuda = os.path.join('./official-converted', 'cuda')\n",
        "g_clone = load_generator(g_params=None, is_g_clone=True, ckpt_dir=ckpt_dir_cuda, custom_cuda=True)\n",
        "g_clone.trainable = False\n",
        "from stylegan2.utils import postprocess_images\n",
        "from stylegan2.utils import adjust_dynamic_range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3-IEYQ6xkRX"
      },
      "source": [
        "import scipy.ndimage\n",
        "import dlib\n",
        "\n",
        "#######################\n",
        "#                     #\n",
        "#                     #\n",
        "#   face alignment    #\n",
        "#                     #\n",
        "#                     #\n",
        "#######################\n",
        "\"\"\"\n",
        " ref:https://github.com/pbaylies/stylegan-encoder\n",
        "\"\"\"\n",
        "\n",
        "class LandmarksDetector:\n",
        "    def __init__(self, predictor_model_path):\n",
        "        \"\"\"\n",
        "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
        "        \"\"\"\n",
        "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
        "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
        "\n",
        "    def get_landmarks(self, image):\n",
        "        img = dlib.load_rgb_image(image)\n",
        "        dets = self.detector(img, 1)\n",
        "\n",
        "        for detection in dets:\n",
        "            try:\n",
        "                face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
        "                yield face_landmarks\n",
        "            except:\n",
        "                print(\"Exception in get_landmarks()!\")\n",
        "\n",
        "def image_align(src_file, dst_file, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True, x_scale=1, y_scale=1, em_scale=0.1, alpha=False):\n",
        "        # Align function from FFHQ dataset pre-processing step\n",
        "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "\n",
        "        lm = np.array(face_landmarks)\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        x *= x_scale\n",
        "        y = np.flipud(x) * [-y_scale, y_scale]\n",
        "        c = eye_avg + eye_to_mouth * em_scale\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        if not os.path.isfile(src_file):\n",
        "            print('\\nCannot find source image.')\n",
        "            return\n",
        "        img = Image.open(src_file).convert('RGBA').convert('RGB')\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = np.uint8(np.clip(np.rint(img), 0, 255))\n",
        "            if alpha:\n",
        "                mask = 1-np.clip(3.0 * mask, 0.0, 1.0)\n",
        "                mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))\n",
        "                img = np.concatenate((img, mask), axis=2)\n",
        "                img = Image.fromarray(img, 'RGBA')\n",
        "            else:\n",
        "                img = Image.fromarray(img, 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), Image.QUAD, (quad + 0.5).flatten(), Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        img.save(dst_file, 'PNG')\n",
        "\n",
        "\n",
        "def align_folder(RAW_IMAGES_DIR='./raw_images/', ALIGNED_IMAGES_DIR='./aligned/'):\n",
        "    landmarks_detector = LandmarksDetector('./shape_predictor_68_face_landmarks.dat')\n",
        "    for img_name in os.listdir(RAW_IMAGES_DIR):\n",
        "            try:\n",
        "                raw_img_path = os.path.join(RAW_IMAGES_DIR, img_name)\n",
        "                fn = face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], 1)\n",
        "                if os.path.isfile(fn):\n",
        "                    continue\n",
        "                for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
        "                    try:\n",
        "                        face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], i)\n",
        "                        aligned_face_path = os.path.join(ALIGNED_IMAGES_DIR, face_img_name)\n",
        "                        image_align(raw_img_path, aligned_face_path, face_landmarks, output_size=1024, x_scale=1, y_scale=1)\n",
        "                    except:\n",
        "                        print(\"Exception in face alignment!\")\n",
        "            except:\n",
        "                print(\"Exception in landmark detection!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpfl6HlXuPWJ"
      },
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "uploaded = files.upload() \n",
        "for k,v in uploaded.items():\n",
        "    shutil.move('./{}'.format(k), './raw_images/{}'.format(k))\n",
        "align_folder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9REHOGmy2Kn"
      },
      "source": [
        "#######################\n",
        "#                     #\n",
        "#                     #\n",
        "#    GAN Inversion    #\n",
        "#                     #\n",
        "#                     #\n",
        "#######################\n",
        "\"\"\"\n",
        "    ref:image2stylegan https://arxiv.org/abs/1904.03189\n",
        "\"\"\"\n",
        "def postprocess_images(images):\n",
        "    # stylegan out\n",
        "    images = adjust_dynamic_range(images, range_in=(-1.0, 1.0), range_out=(0.0, 255.0), out_dtype=tf.dtypes.float32)\n",
        "    images = tf.transpose(images, [0, 2, 3, 1])\n",
        "    return images\n",
        "def genenrate_from_w(w):\n",
        "    # 1, 18, 512 -> 1, 1024, 1024, 3\n",
        "    return postprocess_images(g_clone.generate_from_w_from_wspace(w))\n",
        "def preposccess(x, img_shape=256):\n",
        "    # 1024 -> img_shape\n",
        "    return tf.keras.applications.vgg16.preprocess_input(tf.image.resize(x, (img_shape, img_shape)))\n",
        "def VGG_model(img_shape=256):\n",
        "    # p loss\n",
        "    input_tensor = tf.keras.Input(shape=(img_shape, img_shape, 3))\n",
        "    VGG = tf.keras.applications.VGG16(include_top=False, weights=None, input_tensor=input_tensor)\n",
        "    VGGTF = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
        "    for new_layer, layer in zip(VGG.layers[1:], VGGTF.layers[1:]):\n",
        "        new_layer.set_weights(layer.get_weights())\n",
        "    out1 = tf.keras.layers.Flatten()(VGG.get_layer(\"block1_conv1\").output)\n",
        "    out2 = tf.keras.layers.Flatten()(VGG.get_layer(\"block1_conv2\").output)\n",
        "    out3 = tf.keras.layers.Flatten()(VGG.get_layer(\"block3_conv2\").output)\n",
        "    out4 = tf.keras.layers.Flatten()(VGG.get_layer(\"block4_conv2\").output)\n",
        "    return tf.keras.Model(input_tensor,[out1, out2, out3, out4])\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "@tf.function\n",
        "def projection_step(x, w, model, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(w)\n",
        "        img = genenrate_from_w(w)\n",
        "        img = preposccess(img)\n",
        "        x = preposccess(x)\n",
        "        l2loss = MSE(x, img) \n",
        "        out1, out2, out3, out4 = model(x)\n",
        "        pout1, pout2, pout3, pout4 = model(img)\n",
        "        ploss = MSE(out1 , pout1) + MSE(out2 , pout2) + MSE(out2 , pout2) + MSE(out2 , pout2)\n",
        "        total_loss = l2loss  + ploss \n",
        "    grads = tape.gradient(total_loss, (w, ))\n",
        "    optimizer.apply_gradients(zip(grads, (w, )))\n",
        "    return tf.reduce_mean(total_loss)\n",
        "\n",
        "def projection(x, opt=tf.keras.optimizers.Adam(1e-2, epsilon=1e-08), iters=5000, print_fq=1000):\n",
        "    w = tf.repeat(g_clone.layers[0]([tf.random.normal((1, 512)), None])[:,tf.newaxis,...], 18, axis=1)\n",
        "    w = tf.Variable(w, trainable=True)\n",
        "    vgg = VGG_model()\n",
        "    for i in range(iters):\n",
        "        loss = projection_step(x, w, vgg, opt)\n",
        "        if i % print_fq == 0:\n",
        "            print(\"iter : {}/{},  loss : {} \".format(i, iters, loss.numpy()))\n",
        "    return tf.identity(w.value())\n",
        "\n",
        "w = projection(np.array(Image.open('./aligned/biden_01.png'))[np.newaxis,...].astype(np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9rDexvYxf3R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
